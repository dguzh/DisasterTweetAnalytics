# DisasterTweetAnalytics
## Exploring Changes in Semantic Attributes of Tweets in Response to Natural Disaster Events

**by Aiyana Signer, Diego Gomes, Nathalie Nitsingam & Stylianos Psychias**

This project focuses on how social media responses, specifically those found on Twitter, change in the face of natural disasters. It investigates how semantic attributes of tweets, specifically aggressiveness, sentiment, and stance toward climate change, change when a natural disaster occurs. The process involves the generation of training data from tweets and disaster events, the training of deep learning models, and an analysis of the results.

### Components

**Please refer to individual files for more detailed descriptions, explanations and code demonstrations.**

#### `geoprocessing.py`
This custom Python module provides the core geospatial processing functionality used throughout this project. It defines two main classes: Disaster and TweetRaster. The Disaster class represents a disaster instance, while TweetRaster class helps in rasterizing the Tweet data into a set of pixels, allowing us to handle the data in a spatially aggregated manner.

#### `geoprocessing_demo.ipynb`
This Jupyter notebook demonstrates the functionality of the geoprocessing module, which includes disaster data loading, tweet rasterization, and data preprocessing. It's a great way to understand the core geospatial processing methods.

#### `dataset_compiler.ipynb`
This Jupyter notebook is responsible for generating the training data. It loads tweets and disaster data, rasterizes the tweets into a set of pixels, calculates the semantic changes in response to natural disasters, and exports a variety of training datasets for machine learning models.

#### `model_trainer.ipynb`
This Jupyter notebook is where the deep learning models are defined and trained. Using a Fully Connected Neural Network (or Dense Network), it reads the training datasets created by the `dataset_compiler.ipynb` notebook, trains models on this data, and evaluates their performance. For each semantic attribute, models are created and evaluated using different time windows around a natural disaster event to understand how these time frames affect predictability.

#### `exploratory_analysis.ipynb`
This Jupyter notebook was used to get an overview of the data, learn about the attributes of the datasets, and decide on thresholds that were later used to generate training data and build the models.


### Folders

**Note that if you are reading this document on an offline version of this repository, the folders described in the next section may not be included in the directory due to space limitations. To view them, visit the online repository under the following [link](https://github.com/dguzh/DisasterTweetAnalytics).**

#### `data`
This project relies on a dedicated `data` folder at the root directory level. The folder is designed to house two primary files: `disasters.csv` and `tweets.csv`. Given the substantial file size (> 2GB), these files are not included directly within this repository. Nevertheless, they are readily available and can be downloaded from Kaggle via this [link](https://www.kaggle.com/datasets/deffro/the-climate-change-twitter-dataset?select=The+Climate+Change+Twitter+Dataset.csv).

#### `training_data_[X]deg`
These folders are home to the training datasets generated by the `dataset_compiler.ipynb` notebook. Each folder is labeled in alignment with the resolution of the underlying grid that supports computations for the training sets, with X denoting the equatorial degree spacing defining pixel resolution. Each of these folders contains 48 training sets, representing every possible permutation of the 'days before' and 'days after' parameters for the respective time windows.

#### `models_[X]deg`
These folders serve as storage for the neural network models that are trained using the `model_trainer.ipynb` notebook, utilizing the training datasets from the corresponding `training_data_[X]deg` folder.

#### `model_performance_heatmaps_[X]deg`
These folders host figures visualizing the performance of the models that have been trained using `model_trainer.ipynb`. Each figure presents a 4x4 matrix for each of the four performance metrics: accuracy, precision, recall, and F1 score. These comprehensive visual representations enable effective and clear communication of the model's performance across varying parameters and configurations.