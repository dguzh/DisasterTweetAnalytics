{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eae5bac3-b036-4a77-bcad-618faa5aef5a",
   "metadata": {},
   "source": [
    "# Generating Training Data for Predicting Semantic Changes in Tweets in Response to Natural Disasters\n",
    "\n",
    "In this part of the project, we take the necessary steps to create our training data. This includes loading the tweets and disaster data, rasterizing the tweets into a set of pixels, and calculating the change in semantic attributes in response to natural disaster events.\n",
    "\n",
    "Our data is structured around two main components: Tweets and Disasters. Tweets are geolocated social media posts that contain semantic attributes like 'aggressiveness', 'sentiment', and 'stance'. Disasters are natural disaster events, each associated with a specific geolocation, date and disaster type.\n",
    "\n",
    "To facilitate the analysis, we use a custom module, geoprocessing, which we developed for this project. This module provides two classes: the Disaster class and the TweetRaster class. The Disaster class represents a disaster instance and the TweetRaster class enables us to rasterize the Tweet data into a set of pixels, allowing us to handle the data in a spatially aggregated manner.\n",
    "\n",
    "### Rasterizing Tweets and Calculating Semantic Changes\n",
    "The rasterization process groups tweets based on their geographic location. Each group of tweets is represented by a pixel. This allows us to aggregate the semantic attributes of tweets by pixel and monitor their change in response to natural disasters.\n",
    "\n",
    "Following rasterization, we calculate semantic changes in response to natural disasters. This is done through the create_training_data method, which iterates over all disaster events and calls the compute_change method on each one of them. For each pixel with a minimum of 30 tweets both before and after the event, it calculates whether the semantic attributes of the tweets change significantly using a t-test for the continuous sentiment values and a Chi-Square test for homogeneity for the discrete values for aggressiveness and stance. If there is a significant change, it further identifies whether it is positive or negative.\n",
    "\n",
    "The time frame surrounding the disaster event in which we look for changes can be specified. This flexibility allows us to investigate the impact of disasters over different time periods.\n",
    "\n",
    "The training set generated through this process contains, for each pixel, the change response (no change, positive change, negative change) to each disaster event in the disaster file. This forms the data we use to train our machine learning model.\n",
    "\n",
    "### Geocoding the Disaster Dataset\n",
    "Before we start rasterizing the tweets and calculating semantic changes, it's important to make sure that all disaster events have geographic coordinates. For this, we provide the geocode_disaster_dataset method. This method takes a raw disaster dataset and geocodes the disaster events that have missing coordinates based on their 'Country' column. It then creates a new file with these geocoded events, and from then on, we work with the geocoded version of the file.\n",
    "\n",
    "### Running the Data Generation Process\n",
    "We run the data generation process for each semantic attribute ('aggressiveness', 'sentiment', 'stance') and for different numbers of days before and after the disaster event. This results in 16 different training sets for each attribute, giving us a rich dataset to train and test our models on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "108ef642-70aa-416e-a230-89a6634b7267",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geoprocessing import TweetRaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "638accaf-2081-40f4-bc2d-ccf3e2688cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set raster resolution in Equator degress\n",
    "resolution = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e83d63f-1d33-4753-b83d-7576448bca18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TweetRaster object with the specified resolution\n",
    "raster = TweetRaster(resolution=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8630d47-3ef5-4e1d-aa00-1ac1f13fb75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tweets dataset into the raster object\n",
    "raster.load_tweets('data/tweets.csv',\n",
    "                   longitude_column='lng',\n",
    "                   latitude_column='lat',\n",
    "                   crs=4326,\n",
    "                   filter_before='2015-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93a1b035-05e7-4225-a987-7ef7b49f8e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the date of the most recent tweet\n",
    "latest_tweet = raster.tweets['created_at'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c77751d-4cb2-46ea-8e35-e25c07572e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raster.geocode_disaster_dataset('data/disasters.csv', longitude_column='Longitude', latitude_column='Latitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b61cb8cc-5335-49ca-87a2-925fe44738ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the geocoded disaster dataset into the raster object\n",
    "raster.load_disasters('data/disasters_geocoded.csv',\n",
    "                      longitude_column='Longitude',\n",
    "                      latitude_column='Latitude',\n",
    "                      crs=4326,\n",
    "                      filter_before='2015-01-01',\n",
    "                      filter_after=latest_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fe620b6-c9d3-42b2-be21-f55f421bb800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the semantic attributes and days before and after disaster to consider\n",
    "attributes = ['aggressiveness', 'sentiment', 'stance']\n",
    "days_before_after = [7, 14, 21, 28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "051a8801-1e89-4573-85fd-cb5c77c46206",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ready: training_data_2deg/training_data_2deg_aggressiveness_7db_7da.csv\n",
      "File ready: training_data_2deg/training_data_2deg_aggressiveness_7db_14da.csv\n",
      "File ready: training_data_2deg/training_data_2deg_aggressiveness_7db_21da.csv\n",
      "File ready: training_data_2deg/training_data_2deg_aggressiveness_7db_28da.csv\n",
      "File ready: training_data_2deg/training_data_2deg_aggressiveness_14db_7da.csv\n",
      "File ready: training_data_2deg/training_data_2deg_aggressiveness_14db_14da.csv\n",
      "File ready: training_data_2deg/training_data_2deg_aggressiveness_14db_21da.csv\n",
      "File ready: training_data_2deg/training_data_2deg_aggressiveness_14db_28da.csv\n",
      "File ready: training_data_2deg/training_data_2deg_aggressiveness_21db_7da.csv\n",
      "File ready: training_data_2deg/training_data_2deg_aggressiveness_21db_14da.csv\n",
      "File ready: training_data_2deg/training_data_2deg_aggressiveness_21db_21da.csv\n",
      "File ready: training_data_2deg/training_data_2deg_aggressiveness_21db_28da.csv\n",
      "File ready: training_data_2deg/training_data_2deg_aggressiveness_28db_7da.csv\n",
      "File ready: training_data_2deg/training_data_2deg_aggressiveness_28db_14da.csv\n",
      "File ready: training_data_2deg/training_data_2deg_aggressiveness_28db_21da.csv\n",
      "File ready: training_data_2deg/training_data_2deg_aggressiveness_28db_28da.csv\n",
      "File ready: training_data_2deg/training_data_2deg_sentiment_7db_7da.csv\n",
      "File ready: training_data_2deg/training_data_2deg_sentiment_7db_14da.csv\n",
      "File ready: training_data_2deg/training_data_2deg_sentiment_7db_21da.csv\n",
      "File ready: training_data_2deg/training_data_2deg_sentiment_7db_28da.csv\n",
      "File ready: training_data_2deg/training_data_2deg_sentiment_14db_7da.csv\n",
      "File ready: training_data_2deg/training_data_2deg_sentiment_14db_14da.csv\n",
      "File ready: training_data_2deg/training_data_2deg_sentiment_14db_21da.csv\n",
      "File ready: training_data_2deg/training_data_2deg_sentiment_14db_28da.csv\n",
      "File ready: training_data_2deg/training_data_2deg_sentiment_21db_7da.csv\n",
      "File ready: training_data_2deg/training_data_2deg_sentiment_21db_14da.csv\n",
      "File ready: training_data_2deg/training_data_2deg_sentiment_21db_21da.csv\n",
      "File ready: training_data_2deg/training_data_2deg_sentiment_21db_28da.csv\n",
      "File ready: training_data_2deg/training_data_2deg_sentiment_28db_7da.csv\n",
      "File ready: training_data_2deg/training_data_2deg_sentiment_28db_14da.csv\n",
      "File ready: training_data_2deg/training_data_2deg_sentiment_28db_21da.csv\n",
      "File ready: training_data_2deg/training_data_2deg_sentiment_28db_28da.csv\n",
      "File ready: training_data_2deg/training_data_2deg_stance_7db_7da.csv\n",
      "File ready: training_data_2deg/training_data_2deg_stance_7db_14da.csv\n",
      "File ready: training_data_2deg/training_data_2deg_stance_7db_21da.csv\n",
      "File ready: training_data_2deg/training_data_2deg_stance_7db_28da.csv\n",
      "File ready: training_data_2deg/training_data_2deg_stance_14db_7da.csv\n",
      "File ready: training_data_2deg/training_data_2deg_stance_14db_14da.csv\n",
      "File ready: training_data_2deg/training_data_2deg_stance_14db_21da.csv\n",
      "File ready: training_data_2deg/training_data_2deg_stance_14db_28da.csv\n",
      "File ready: training_data_2deg/training_data_2deg_stance_21db_7da.csv\n",
      "File ready: training_data_2deg/training_data_2deg_stance_21db_14da.csv\n",
      "File ready: training_data_2deg/training_data_2deg_stance_21db_21da.csv\n",
      "File ready: training_data_2deg/training_data_2deg_stance_21db_28da.csv\n",
      "File ready: training_data_2deg/training_data_2deg_stance_28db_7da.csv\n",
      "File ready: training_data_2deg/training_data_2deg_stance_28db_14da.csv\n",
      "File ready: training_data_2deg/training_data_2deg_stance_28db_21da.csv\n",
      "File ready: training_data_2deg/training_data_2deg_stance_28db_28da.csv\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "\n",
    "# Loop over each attribute and each combination of days before and after\n",
    "for attribute in attributes:\n",
    "    for days_before in days_before_after:\n",
    "        for days_after in days_before_after:\n",
    "            \n",
    "            # Construct the path of the training data file\n",
    "            PATH = f'training_data_{resolution}deg/training_data_{resolution}deg_{attribute}_{days_before}db_{days_after}da.csv'\n",
    "            \n",
    "            # Check if the training data file already exists\n",
    "            if not os.path.exists(PATH):\n",
    "                # Create the training data\n",
    "                raster.create_training_data(attribute, days_before, days_after)\n",
    "                # Write the training data to a CSV file\n",
    "                raster.write_training_data(attribute, days_before, days_after)\n",
    "                \n",
    "            print(f'File ready: {PATH}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
